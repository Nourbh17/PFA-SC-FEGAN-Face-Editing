# -*- coding: utf-8 -*-
"""Discriminator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_0sEQIhNlhUXxz3M5cUWP99jn1ZPTLer
"""

import tensorflow as tf

class conv2d_layer(tf.keras.layers.Layer):
    def __init__(self, out_channels, kernel_size, stride=1, padding='valid', dilation=1, pad_type='zero', activation='lrelu', norm='none', sn=False):
        super(conv2d_layer, self).__init__()


        # Initialize the padding scheme
        if pad_type == 'reflect':
            self.pad = tf.keras.layers.Lambda(lambda x: tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT'))
        elif pad_type == 'replicate':
            self.pad = tf.keras.layers.Lambda(lambda x: tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='SYMMETRIC'))
        elif pad_type == 'zero':
            self.pad = tf.keras.layers.ZeroPadding2D(padding)
        else:
            assert 0, "Unsupported padding type: {}".format(pad_type)

        # Initialize the normalization type
        if norm == 'bn':
            self.norm = tf.keras.layers.BatchNormalization()
        elif norm == 'in':
            self.norm = tf.keras.layers.LayerNormalization()
        elif norm == 'ln':
            self.norm = tf.keras.layers.LayerNormalization(axis=[1, 2])
        elif norm == 'none':
            self.norm = None
        else:
            assert 0, "Unsupported normalization: {}".format(norm)

        # Initialize the activation function
        if activation == 'relu':
            self.activation = tf.keras.layers.ReLU()
        elif activation == 'lrelu':
            self.activation = tf.keras.layers.LeakyReLU(0.2)
        elif activation == 'prelu':
            self.activation = tf.keras.layers.PReLU()
        elif activation == 'selu':
            self.activation = tf.keras.layers.SELU()
        elif activation == 'tanh':
            self.activation = tf.keras.activations.tanh
        elif activation == 'sigmoid':
            self.activation = tf.keras.activations.sigmoid
        elif activation == 'none':
            self.activation = None
        else:
            assert 0, "Unsupported activation: {}".format(activation)

        # Initialize the convolution layers
        if sn:
            self.conv2d = SpectralNorm(tf.keras.layers.Conv2D(out_channels, kernel_size, strides=stride, padding='valid', dilation_rate=dilation, use_bias=False))
        else:
            self.conv2d = tf.keras.layers.Conv2D(out_channels, kernel_size, strides=stride, padding='valid', dilation_rate=dilation, use_bias=False)

    def call(self, x):
        x = self.pad(x)
        x = self.conv2d(x)
        if self.norm:
            x = self.norm(x)
        if self.activation:
            x = self.activation(x)
        return x

def patch_discriminator(img, mask, opt):
    x = tf.concat([img, mask], axis=-1)
    x = conv2d_layer(x, 64, 3, 1, padding='same', activation=tf.nn.leaky_relu, use_sn=True)
    x = conv2d_layer(x, 128, 3, 2, padding='same', activation=tf.nn.leaky_relu, use_sn=True, use_bn=True)
    x = conv2d_layer(x, 256, 3, 2, padding='same', activation=tf.nn.leaky_relu, use_sn=True, use_bn=True)
    x = conv2d_layer(x, 512, 3, 2, padding='same', activation=tf.nn.leaky_relu, use_sn=True, use_bn=True)
    x = conv2d_layer(x, 512, 3, 2, padding='same', activation=tf.nn.leaky_relu, use_sn=True, use_bn=True)
    x = conv2d_layer(x, 1, 3, 2, padding='same', activation=None, use_sn=True)
    return x

class SpectralNorm(tf.keras.layers.Wrapper):
  def __init__(self, layer, iteration=1, name="weight", **kwargs):
    super(SpectralNorm, self).__init__(layer, **kwargs)
    self.iteration = iteration
    self.name = name

  def build(self, input_shape):
    if not hasattr(self.layer, self.name):
      raise ValueError(f"Layer {self.layer.name} does not have attribute {self.name}")
    w = getattr(self.layer, self.name)
    self.w_bar = self.add_weight(
        shape=w.shape,
        name=f"{self.name}_bar",
        initializer=tf.keras.initializers.TruncatedNormal(stddev=1.0),
        trainable=True
    )
    self.u = self.add_weight(
        shape=(w.shape[0], 1),
        name=f"{self.name}_u",
        initializer=tf.random.normal_initializer(mean=0.0, stddev=1.0),
        trainable=False
    )
    self.v = self.add_weight(
        shape=(1, w.shape[1]),
        name=f"{self.name}_v",
        initializer=tf.random.normal_initializer(mean=0.0, stddev=1.0),
        trainable=False
    )
    super(SpectralNorm, self).build(input_shape)

  def call(self, inputs, training=None):
    if training:
      self._update_u_v(inputs)
    sigma = tf.tensordot(self.u, tf.matmul(self.w_bar, self.v), axes=([0], [1]))
    self.layer.setattr(self.name, self.w_bar / sigma)
    return self.layer(inputs)

  def _update_u_v(self, inputs):
    w_reshaped = tf.reshape(self.w_bar, (-1, self.w_bar.shape[1]))
    for _ in range(self.iteration):
      v = tf.nn.l2_normalize(tf.matmul(tf.transpose(w_reshaped), self.u), axis=0)
      self.u.assign(tf.nn.l2_normalize(tf.matmul(w_reshaped, v), axis=1))