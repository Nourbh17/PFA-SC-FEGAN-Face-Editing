# -*- coding: utf-8 -*-
"""training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DVD1ulyuAqI_InHSSg73ZQ3YK05TN-bS
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/SC-FEGAN"

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/PFA"

!pip install import-ipynb

import import_ipynb
import os
import cv2 as cv
import matplotlib.pyplot as plt
import numpy as np
import os
import time
import tensorflow as tf
from keras.layers import Input
from tqdm import tqdm
from tensorflow.keras.optimizers import Adam
from Discriminator import Discriminator
from Generator import Generator
from DiscriminatorLosses import DiscrimnatorLosses
from GeneratorLosses import GeneratorLosses

def model():
    gen_input = Input(shape=(512, 512, 9))
    dis_input = Input(shape=(512, 512, 8))
    model_generator = Generator()
    model_disciminator = Discriminator()
    gen_model = model_generator.call(gen_input)
    dis_model = model_disciminator.call(dis_input)
    return gen_model, dis_model

model_generator,model_discriminator = model()

generator_optimizer = Adam(1e-4, beta_1=0.1, beta_2=0.999)
discriminator_optimizer = Adam(1e-4, beta_1=0.1, beta_2=0.999)

def data_split(data):
    input_gen = data.total_input
    ground_truth = data.ground_truth
    batch_data = data.batch_data

    incomplete_image,sketch ,color ,mask,noise = batch_data[0],batch_data[1],batch_data[2],batch_data[3],\
                                                 batch_data[4]

    return input_gen,ground_truth,incomplete_image,sketch,color,mask,noise



def data_distrution(input):
    data=tf.data.Dataset.from_tensor_slices((input)).batch(6,drop_remainder=True)
    return data


input_gen,ground_truth,incomplete_image,sketch,color,mask,noise=data_split(data)


input_data = data_distrution(input_gen)
incomplete_data = data_distrution(incomplete_image)
mask_data = data_distrution(mask)
real_data = data_distrution(ground_truth)
sketch_data = data_distrution(sketch)
color_data = data_distrution(color)

def apply_gradient(input_gen,mask,incomplete_image,ground_truth,sketch,color):
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:

        output_gen = model_generator(input_gen)

        complete_image = incomplete_image + (mask * output_gen)

        batch_pos= tf.concat([ground_truth,sketch,color,mask],axis=-1)
        batch_neg = tf.concat([complete_image,sketch,color,mask], axis=-1)

        dis_real = model_discriminator(batch_pos)
        dis_fake = model_discriminator(batch_neg)

        gen_loss = GeneratorLosses.generator_loss(output_gen, ground_truth, complete_image, mask, dis_fake, dis_real)
        dis_loss = DiscrimnatorLosses.total_dis_loss(dis_real,dis_fake,model_discriminator,batch_pos,batch_neg,mask)

        generator_gradients = gen_tape.gradient(gen_loss,model_generator.trainable_variables)
        discriminator_gradients = disc_tape.gradient(dis_loss,model_discriminator.trainable_variables)

        generator_optimizer.apply_gradients(zip(generator_gradients,model_generator.trainable_variables))
        discriminator_optimizer.apply_gradients(zip(discriminator_gradients,model_discriminator.trainable_variables))


        return output_gen,gen_loss,dis_loss

def train_data():
    dis_losses = []
    gen_losses = []
    for step, data in enumerate(zip(real_data, input_data, incomplete_data, mask_data, sketch_data, color_data)):
        output_gen, gen_loss, dis_loss = apply_gradient(data[1],data[3],data[2],data[0],data[4],data[5])


        gen_losses.append(gen_loss.numpy())
        dis_losses.append(dis_loss.numpy())

        print(f"Training loss for step_num {step} ,gen_loss:{gen_loss:0.4f},dis_loss:{dis_loss:0.4f}")


    return gen_losses,dis_losses

def fit(epochs=1):
    epochs_gen_losses, epochs_dis_losses = [], []
    for epoch in range(epochs):
        print(f"Start of epoch number : {epoch+1}")
        start_time = time.time()
        gen_losses,dis_losses = train()
        elapsed = time.time() - start_time
        minutes = int(elapsed // 60)
        seconds = int(elapsed % 60)
        epochs_gen_losses.append(np.mean(gen_losses))
        epochs_dis_losses.append(np.mean(dis_losses))
        print(f'Epoch {epoch+1}: gen_loss: {np.mean(gen_losses):0.3f}  dis_loss: {np.mean(dis_losses):.3f} time:({minutes} min {seconds} sec)')
        os.mkdir(f"model_weights/epoch_{epoch+1}")
        model_generator.save_weights(f"model_weights/epoch_{epoch+1}/gen_model_epoch_{epoch+1}.h5")
        model_discriminator.save_weights(f"model_weights/epoch_{epoch+1}/dis_model_epoch_{epoch+1}.h5")
        genereta_random_numper =np.random.randint(0,15)
        # data_evaluation(genereta_random_numper)

    # plot_history(epochs_gen_losses,epochs_dis_losses,epochs)